---
title: "Homework 3"
author: "Solution"
date: "25 February 2020"
output:
  html_document:
    fig_caption: yes
    theme: flatly
  pdf_document:
    includes:
      in_header: ../support/432macros.tex
    number_sections: no
    template: ../support/dm-docs.tex
    toc: no
---

```{r, include=FALSE}
# General set-up for the report:
# Don't print out code
# Save results so that code blocks aren't re-run unless code
# changes (cache), _or_ a relevant earlier code block changed (autodep),
# don't clutter R output with messages or warnings (message, warning)
library(MASS)
library(knitr)
library(tidyverse)
opts_chunk$set(echo=FALSE, fig.align = TRUE, fig.height = 4, fig.width = 6,
               cache=TRUE, autodep=TRUE,
               message=FALSE, warning=FALSE)
# Turn off meaningless clutter in summary() output
options(show.signif.stars=FALSE)
```

## Introduction

Appraising residential real estate --- predicting the price at which it could
be sold, under current market conditions --- is important not only for people
buying and selling houses to live in, but also for real estate developers,
mortgage lenders, and local tax assessors.  Currently, appraisal is usually
done by skilled professionals who make a good living at it, so naturally there
is interest in replacing them by machines.  In this report, we investigate the
feasibility of real estate appraisal by means of linear statistical models.

Specific points of interest to the client include the relationship between the
quality of the house's construction and its price; the relationship between age
and price, and whether this changes depending on proximity to a highway; and
the relationship between price, the finished area of the house, and the number
of bedrooms.

## Exploratory data analysis

```{r, include=FALSE}
# Load the data
real_estate <- read_csv(
  "http://www.stat.cmu.edu/~cshalizi/mreg/15/hw/08/real-estate.csv",
  col_types = "dddiififdfdf") %>% 
  mutate(
    Quality = fct_recode(Quality, High = "1", Medium = "2", Low = "3"),
    AC = fct_recode(Airconditioning, Yes = "1", No = "0"),
    Pool = fct_recode(Pool, Yes="1", No="0"),
    Highway = fct_recode(as.factor(AdjHighway), Adjacent="1", Not = "0")
  )

```

The data, supplied by an undisclosed client, come from a selection of
"arms-length" residential real estate transactions in an unnamed city in the
American midwest in 2002.  This records, for `r nrow(real_estate)`
transactions, the sale price of the house, its finished area and the area of
the lot, the number of bedrooms, the number of bathrooms, the number of cars
that will fit in its garage, the year it was built, whether it has air
conditioning, whether it has a pool, whether it is adjacent to a highway, and
the quality of construction, graded from low to medium or high.  It is notable
that, except for highway adjacency, we have no information about the location
of the houses, though this is proverbially a very important influence on their
price, through access to schools, commuting time, land value, etc.

```{r, pairsplot, fig.width=8, fig.height=8, fig.cap="Pairs plot for quantitative variables"}
# Base R version
# pairs(Price ~ Sqft+Bedroom+Bathroom+Garage+YearBuild+Lot, data=real.estate,
#      pch=19, cex=0.3)

# Pretty version
library(GGally)
library(cowplot)
ggpairs(
  real_estate, 
  c("Price", "Sqft", "Bedroom","Bathroom","Garage", "YearBuild", "Lot"),
  upper="blank", 
  lower = list(continuous = wrap("points", alpha=.4, color="purple"))
  ) + theme_cowplot(8)
```


Pairwise scatter-plots for the quantitative variables (Figure 1) show that,
unsurprisingly, there is a positive relationship between price and area
(stronger for finished area than the total lot size), and price and the number
of bedrooms, bathrooms, or garage slots (all three of which are strongly
positively related to each other).  The relation between price and these three
"count" variables could well be linear.  There is a positive relation between
price and the year of construction, i.e., newer houses cost more.  Newer houses
also tend to be larger, both in finished area and the number of rooms, though
not to have bigger lots.

Inspection of the plots shows there is one record with 0 bedrooms, 0 bathrooms,
and a three-car garage with air conditioning.  This is either not a piece of
residential real estate, or its data is hopelessly corrupt; either way, we drop
it from the data from now on.

```{r}
# What's that outlier with zero bedrooms and zero bathrooms?
  # From visual inspection of pairs plot, no other weirdness with houses without
  # bedrooms or bathrooms
# bad.row <- which(real.estate$Bedroom==0 & real.estate$Bathroom==0)
# Examination of real.estate[bad.row,] shows Garage==3, Airconditioning==1, etc.
# Remove the bad row
real_estate <- dplyr::filter(real_estate, !(Bedroom == 0 & Bathroom==0))
```


```{r, price-given-discrete-predictors-gg, fig.cap="Conditional distributions of price given qualitative predictors. Box widths reflect the number of points in each group, notches show medians plus/minus a margin of error."}
real_estate %>% 
  dplyr::select(Price, AC, Pool, Highway, Quality) %>% 
  pivot_longer(-Price) %>%
  ggplot(aes(y=Price/10000, x=value, fill=value)) + geom_boxplot() + 
  facet_wrap(~name, ncol=2, scales='free_x') +
  theme_cowplot(12) + theme(legend.position = 'none') + ylab("Price ($10000s)") + 
  scale_fill_brewer(palette = "Set1")
```


Box-plots, showing the conditional distribution of price for each level of the
categorical predictors, suggest that houses with air-conditioning and pools are
more expensive, that being next to a highway makes little difference, and that
higher quality of construction implies, on average, higher prices.  The
mid-points of the boxes for quality don't _quite_ fall on a straight line, so
treating quality as a numerical variable isn't obviously compelling, but not
clearly crazy either.

## Initial Modeling

To answer the client's questions, our model should include quality, finished
area, the number of bedrooms (and the interaction between those two), and the
year the house was built and whether it is adjacent to a highway (and the
interaction between those two).  Based on our EDA, it also seems reasonable to
include air-conditioning and pools.  We deliberately left out the number of
bathrooms, the size of the garage, and the size of the lot.  While price seems
to be linearly related to the number of bedrooms, we include it as a factor,
both to check that, and to get three distinct slopes for price on finished area
as quality varies.

```{r, include=FALSE}
initial_mdl <- lm(Price ~ Quality + Sqft*factor(Bedroom)
                  + YearBuild*AdjHighway + Airconditioning + Pool,
                  data=real_estate)
```

This initial model has a root-mean-squared error of \$ $\pm
`r signif(summary(initial_mdl)$sigma, 3)`$, which is not shabby when the median
house price is \$ $`r signif(median(real_estate$Price),3)`$.  Before passing to
issues of model selection, however, such as whether all the interactions are
necessary, whether discrete variables might be usefully recoded, etc., let's
look at the diagnostic plots.

The first thing to say is that the distribution of the residuals doesn't
look very Gaussian, and a Box-Cox transformation suggests the un-intuitive,
indeed  un-interpretable, transformation $1/\sqrt[3]{Y}$.


```{r,qq-and-boxcox-plots-gg, eval=FALSE, out.height="0.2\\textheight", fig.align="center", fig.cap="Q-Q plot of the standardized residuals (left) and Box-Cox plot (right)"}
qq = ggplot(data.frame(residuals = rstandard(initial_mdl)), aes(sample=residuals)) + 
  geom_qq(color="purple") + geom_qq_line(color="orange",size=2) + theme_cowplot()
bc = boxcox(initial.mdl, plotit = FALSE)
ran = bc$x[bc$y > max(bc$y) - 0.5*qchisq(.95,1)]
bcplot = ggplot(data.frame(loglikelihood = bc$y, lambda = bc$x), aes(lambda, loglikelihood)) +
  geom_line(color="purple") + geom_vline(xintercept = bc$x[which.max(bc$y)], color="orange", size=2) +
  geom_vline(xintercept = c(min(ran),max(ran)), color="orange", linetype="dotted") + theme_cowplot()
plot_grid(qq, bcplot)
```

Clients who ask for a model of prices are rarely happy with models for the
inverse cubic roots of prices, so we must be doing something wrong.  Examining
plots of residuals versus predictors suggests that lot size matters after all,
at least for big lots.  The plots also suggest that houses built after $\approx
1980$ are worth more than the model anticipates.  The distributions of
residuals conditional on discrete predictors, however, actually look mostly
homogeneous.

```{r, include=FALSE}
# Function to plot residuals and fitted values vs. predictors
# Note the similarity to the function from hw1
# Inputs: 
#     mdl - a fitted lm model; 
#     dataframe - the name of the data frame; 
#     pred_list - quoted names of continuous predictors
# Output: returns a ggplot object
resid_vs_pred <- function(mdl, dataframe, pred_list) {
    df = dataframe[pred_list]
    df$resids = residuals(mdl) # how do you get residuals?
    df$fitted_values = fitted(mdl) # how do you get the fitted values?
    preds_resids = df %>% 
      pivot_longer(-resids, names_to = 'predictor', values_to = 'value')
    
    ggplot(preds_resids, aes(x=value, y=resids)) + geom_point() +
      geom_smooth() + facet_wrap(~predictor, scales = 'free_x') +
      geom_hline(yintercept = 0, color="red") +
      geom_hline(yintercept = c(2*sd(df$resids),-2*sd(df$resids)),
                 color="grey", linetype="dashed") +
      theme_cowplot(8) + xlab('')
}
```

```{r, diagnostic-plots, fig.height=6, fig.cap=" Residuals versus fitted values and continuous predictors, and versus the discrete predictors.  Grey lines are smoothing splines; dotted lines indicate plus/minus 2 standard deviations, either constant (red) or from a spline smoothing of the squared residuals (grey)."}
cont_resids = resid_vs_pred(
  initial_mdl, real_estate, c("Sqft","YearBuild"))
bxplt_dat = real_estate %>% 
  select(Bedroom, Bathroom, Garage, Highway, Quality, AC) %>%
  mutate_at(vars(Bedroom, Bathroom, Garage), as.factor)
bxplt_dat$residuals = rstandard(initial_mdl)
bxplt_dat = bxplt_dat %>% pivot_longer(-residuals)
bxplts = ggplot(bxplt_dat, aes(y=residuals, x=value, fill=value)) +
  geom_boxplot() + facet_wrap(~name, scales = "free_x") +
  scale_fill_viridis_d() +
  theme_cowplot(8) + theme(legend.position = 'none') + xlab('')
plot_grid(cont_resids, bxplts, nrow = 2, rel_heights = c(1.25,2))
```



## Outliers

In addition to the house with no bedrooms or bathrooms, examination of Cook's
distance shows two houses with exceptional influence over the model.

```{r, cooks-plot, out.height="0.25\\textheight", fig.align="center", fig.cap="Cook's distance for each data point: extremely influential points are flagged in red."}
ggplot(
  tibble(cook = cooks.distance(initial_mdl),
         bad_houses = cook > 0.1,
         price = real_estate$Price),
  aes(price/10000, cook, color=bad_houses)) + geom_point() +
  theme_cowplot() + ylab("Cook's Distance") +
  theme(legend.position = c(.75,.75)) +
  scale_color_manual(values=c("purple","orange")) +
  labs(color = "Bad houses") + xlab("price ($10000s)")
bad_houses <- which(cooks.distance(initial_mdl)>0.1)
```

On examination, these are quite weird: small in area, fairly cheap, but heavy
on bedrooms.  These look more like rental properties than residences.  Checking
the pairs plot again shows no other such anomalies, so we delete them but
leave the rest alone.  Re-doing the other diagnostic plots shows little
over-all change, however (figures omitted).
```{r}
kable(real_estate[bad_houses,c("Price","Sqft","Bedroom","Bathroom")])
```

```{r}
real_estate <- real_estate[-bad_houses,]
```

## Model selection / evaluation

Our examination of the data and the diagnostics plots suggest several possible
modifications to our baseline model: adding a term for lot size, letting the
slope on year change for younger houses, recoding bedrooms to the three levels,
and turning the interactions of interest off.  Each of these five choices
is logically independent of the others, yielding $2^5=32$ possible models.

Rather than doing 32 sets of diagnostics, we will use leave-one-out
cross-validation to select a model.  That is, we'll fit each model on $n-1$ of
the data points, see how well they predict the $n^{\mathrm{th}}$ data point
without having seen it, and average squared errors across data points for each
model.  This gives us a good estimate of how well the models would predict new
data, which is ultimately what the client cares about.

Since we also want to do statistical inference on our selected model, however,
we will run the cross-validation on only _half_ of the data; the other half
will be used just for inference on the final, selected model.  If we used the
same data twice, for both selection and for inference, we'd exaggerate the
precision of our inferences, basically because we'd be asking how well our
model fit the data it was selected to fit.  Splitting the data by taking a
random sample of half the data points and keeping it aside for inference avoids
this problem.

```{r, add-new-vars}
# Before model selection, we need to create the recoded bedrooms variable
# Three levels: 1 bedroom, 2--4 bedrooms, 5+ bedrooms
real_estate = real_estate %>%
  mutate(BedsCoded = fct_collapse(
    factor(Bedroom), `1` = "1", `2--4` = c("2","3","4"), group_other = TRUE)
    ) %>% 
  mutate(
    BedsCoded = fct_recode(BedsCoded, `5+` = "Other"),
# Also create an extra column to indicate if the house is fairly recent,
# meaning built since 1980
    Since1980 = factor(YearBuild > 1980))
# Not run in script, but as a manual check that this worked right:
### with(real_estate, table(Bedroom, Beds.Coded))

```

```{r, all-the-formulas}
# Create all the model formulae
# We _could_ just type them all out, but that would take so long, and be
# so error-prone, that it's better to do a _little_ automation
# Start with a vector of model formulas
all_the_formulas <- vector(length=32, mode="character")
# Default set of terms
  # Hand-coded here, but with a bit of manipulation of the terms() function
  # this could actually be extracted automatically from formula(initial.mdl)
default_terms <- c("Quality", "Sqft", "factor(Bedroom)",
                   "YearBuild", "AdjHighway", "Airconditioning",
                   "Pool", "Sqft:factor(Bedroom)", "YearBuild:AdjHighway")
# Loop through our choices
counter <- 0 # Keep track of where we are in the list
for (area.by.bedrooms in c(TRUE, FALSE)) {
    for (recode.bedrooms in c(TRUE, FALSE)) {
        for (age.by.highway in c(TRUE,FALSE)) {
            for (after.1980 in c(TRUE, FALSE)) {
                for (lot in c(TRUE, FALSE)) {
                    # Set up for each pass through the loop
                    counter <- counter +1
                    the.terms <- default_terms

                    # For each choice, remove terms we don't want and add
                    # the ones we do; remember that if we change a variable
                    # and it shows up in an interaction, we should change
                    # the interaction as well
                    if (recode.bedrooms) {
                        the.terms <- setdiff(the.terms, "factor(Bedroom)")
                        the.terms <- union(the.terms, "BedsCoded")
                        if (area.by.bedrooms) {
                            the.terms <- setdiff(the.terms,
                                                 "Sqft:factor(Bedroom)")
                            the.terms <- union(the.terms,
                                               "Sqft:BedsCoded")
                        }
                    }
                    if (!area.by.bedrooms) {
                        the.terms <- setdiff(the.terms, "Sqft:factor(Bedroom)")
                    }
                    if (! age.by.highway) {
                        the.terms <- setdiff(the.terms, "YearBuild:AdjHighway")
                    }
                    if (after.1980) {
                        the.terms <- union(the.terms, "YearBuild*Since1980")
                        # if (age.by.highway) {
                        #  the.terms <- union(the.terms,
                        #                    "YearBuild*Since1980*AdjHighway")
                        # }
                    }
                    if (lot) {
                        the.terms <- union(the.terms, "Lot")
                    }

                    # Turn the set of terms into a single string
                      # which then we can use as a formula
                      # See last example in help(formula) for the trick
                      # that follows
                    fmla <- paste("Price ~", paste(the.terms, collapse="+"))
                    all_the_formulas[counter] <- fmla
                }
            }
        }
    }
}
```

```{r, LOOCV}
# Calculate LOOCV score for a linear model
# Input: a model as fit by lm()
# Output: leave-one-out CV score
cv.lm <- function(mdl) {
    return(mean((residuals(mdl)/(1-hatvalues(mdl)))^2))
}
```

```{r, data-splitting}
# Fix the random number seed, to ensure reproducibility
set.seed(2015-11-17)

# Divide the data in two, by random sampling of rows
select.rows <- sample(1:nrow(real_estate), replace=FALSE,
                      size=floor(nrow(real_estate)/2))
# That half of the data set will now be used for selection
select.set <- real_estate[select.rows,]
# The other half will be used for inference later
inference.set <- real_estate[-select.rows,]
```

```{r, fit-on-half}
# Make a list containing all of the estimated models from all the
# different formulas
all_the_models <- lapply(all_the_formulas,
                         function(fmla) { lm(fmla, data=select.set) } )
# Do leave-one-out CV for each model
all_the_LOOCVs <- sapply(all_the_models, cv.lm)
# Pick the best
best.index <- which.min(all_the_LOOCVs)
# It can also be helpful to look at the runner-up
second.best.index <- which.min(all_the_LOOCVs[-best.index])
```

```{r, term-examination}
# Look at the difference in terms between the best model and our initial
# model
  # No particularly nice way to convert from R output to English or a table
  # automatically here, so this is examined by hand

# What, exactly, is the formula of the best model?
best.formula <- formula(all_the_models[[best.index]])
# What terms does it contain?
  # You could just look at it, but this incantation is how R extracts it
  # automatically --- see help pages for formula(), terms() and attr(),
  # in that order
best.terms <- attr(terms(best.formula), "term.labels")
# What terms does the best model add to the baseline?
added <- setdiff(best.terms, default_terms)
# What terms does it drop?
dropped <- setdiff(default_terms, best.terms)
# Similarly for the second-best model
second.best.formula <- formula(all_the_models[-best.index][[second.best.index]])
second.best.terms <- attr(terms(second.best.formula), "term.labels")
second.added <- setdiff(second.best.terms, default_terms)
second.dropped <- setdiff(default_terms, second.best.terms)
```

Compared to our initial model, the best-predicting model replaces the giving
each bedroom number its own contrast with the three-level coding of bedrooms;
adds lot size as a predictor; and lets more-recent (post-1980) houses have
their own slope on the year of construction.  Everything else, including all
the interactions, is the same.  Our confidence that these are mostly good
choices is reinforced by the fact that the second-best model makes all the same
changes to the initial model, except that it drops the interaction between year
of construction and adjacency to the highway.

## Final model/inference

Having selected our model on one half of the data, we may now do statistical
inference on the other half, and not _guarantee_ that the results are invalid.
Table 1 gives point estimates, standard errors, $p$-values, and 95%
confidence intervals for all the coefficients.  This is strongly
suggestive of higher quality construction predicting higher prices,
since both the contrast coefficients for lower quality are negative,
by hundreds of thousands of dollars.

```{r, selected-model-table}
favorite.model <- lm(best.formula, data=inference.set)
favorite.table <- cbind(coefficients(summary(favorite.model)),
                        confint(favorite.model))
kable(signif(favorite.table,3),digits=25)
```

_Table 1: Point estimates and inferential statistics for our selected model._


The client's other specific questions are best answered using figures.  The slope plots (Figure 5, left), shows the contribution made to the predicted price by the finished 
area for each level of the number of bedrooms.  This suggests that, first, more
area predicts a higher price, but, second, at equal area, more bedrooms
predicts a higher price, except _maybe_ for the very largest houses.  I qualify
the conclusion this way because there are very few 2--4 bedroom houses over
about 4000 square feet (and no one-bedroom houses), where the predicted line
for 2--4 bedrooms goes above that of 5+ bedrooms.

Turning to the right panel of the figure, it shows younger houses are predicted
to have higher prices, with the premium on youth turning up fairly sharply if
the house was built after 1980. 




```{r, variable-slope-plots, fig.cap="The contribution of finished area to predicted price, as a function of the number of bedrooms (left), and the contribution of year of construction (relative to a baseline of the year 0, which is why the amounts are so huge), reflecting a changing trend after 1980 (right).  (The estimate is surprisingly smooth, considering we didn't enforce any sort of continuity.)  Rugs along the horizontal axis show the continuous values belonging to each category."}
par(mfrow=c(1,2))
favorite.coefficients <- coefficients(favorite.model)
# Finished area (Sqft) for each level of bedrooms
  # First, default category of 1 bedroom
curve(x*favorite.coefficients["Sqft"],
      from=min(real_estate$Sqft), to=max(real_estate$Sqft),
      xlab="Finished area", ylab="Contribution to price",
      ylim=c(0,4e5))
rug(with(real_estate, Sqft[BedsCoded=="1"]),side=1)
abline(a=favorite.coefficients["BedsCoded2--4"],
       b=favorite.coefficients["Sqft"]+
         favorite.coefficients["Sqft:BedsCoded2--4"], col="blue")
rug(with(real_estate, Sqft[BedsCoded=="2--4"]),side=1, col="blue")
abline(a=favorite.coefficients["BedsCoded5+"],
       b=favorite.coefficients["Sqft"]+
         favorite.coefficients["Sqft:BedsCoded5+"], col="red")
rug(with(real_estate, Sqft[BedsCoded=="5+"]),side=1, col="red")
legend("topleft", legend=paste("Bedrooms:", levels(real_estate$BedsCoded)),
       col=c("black","blue","red"), lty="solid", cex=0.5)

# Year of construction
curve(x*favorite.coefficients["YearBuild"]
      + (x>1980)*favorite.coefficients["Since1980TRUE"]
      + x*(x>1980)*favorite.coefficients["YearBuild:Since1980TRUE"],
      from=min(real_estate$YearBuild), to=max(real_estate$YearBuild),
      xlab="Year of construction", ylab="Contribution to price")
rug(with(real_estate, YearBuild[AdjHighway==0]), side=1)
curve(favorite.coefficients["AdjHighway1"]
      + x*favorite.coefficients["YearBuild"]
      + (x>1980)*favorite.coefficients["Since1980TRUE"]
      + x*(x>1980)*favorite.coefficients["YearBuild:Since1980TRUE"], 
      add=TRUE, col="blue")
rug(with(real_estate, YearBuild[AdjHighway==1]), side=1, col="blue")
legend("topleft", legend=c("Away from highway", "Adjacent to highway"),
       col=c("black","blue"), lty="solid", cex=0.5)
par(mfrow=c(1,1))
```


Relying very much on the exact confidence intervals and $p$-values from Table 1
is a bit dubious.  The model predicts pretty well: its
leave-one-out RMSE is $\pm \$
`r signif(sqrt(cv.lm(lm(formula(favorite.model),data=real_estate))),3)`$,
while that of our original model is
$\pm \$
`r signif(sqrt(cv.lm(initial_mdl)),3)`$.
Moreover, plotting residuals against fitted values shows almost exactly no
trend to the former, and reasonably constant variance (Figure 6, left).  (Plots
of residuals against predictors [omitted] are similarly good.)  But those same
residuals are still not very Gaussian (Figure 6, right).  Thus, while the
inferential statistics tell us that none of the interaction terms are
significantly different from zero, or even estimated to within closer than $\pm
\$ {10}^5$ (except the contrasts for the house having been built since 1980),
we don't know how much trust we can put in those results.  If only we knew a
way of doing inference without assuming Gaussian noise...


```{r, diagnostics-final-model, fig.cap="Diagnostics for the final model."}
par(mfrow=c(1,2))
plot(fitted(favorite.model), residuals(favorite.model), pch=19, cex=0.5,
     xlab="fitted values", ylab="residuals")
qqnorm(rstandard(favorite.model), main="", pch=19, cex=0.5)
qqline(rstandard(favorite.model))
par(mfrow=c(1,1))
```


## Conclusion

We have found a model which predicts the price of houses to within about
$`r signif(sqrt(cv.lm(lm(formula(favorite.model),data=real_estate))),2)`$.  The
coefficients in this model all make sense after we see them [^duncan]:
high-quality construction predicts higher prices, as do amenities like pools
and air-conditioning, as do bigger lots, more floor space, and more bedrooms.
Younger houses command a premium, especially houses built since 1980.  It is
somewhat annoying that the residuals remain not-very-Gaussian, but that's
because our statistical technique is still too weak to do inference under these
conditions, not because the model is wrong.

[^duncan]: Of course, as the sociologist Duncan Watts says, "Everything is obvious, once you know the answer".  His book of that title can hardly be too highly recommended to anyone who is going to spend time interpreting models like these.

Because age (year of building) emerged as one of the strongest predictors, it
would be good to know how it's linked to price.  Perhaps, since neighboring
houses tend to be of similar ages, it's acting as a proxy for location.
Indeed, probably the single biggest thing missing from this data set is
location.  Even without it, though, we can do a tolerable job of rolling
our own Zillow, and could do better with more data.



# Afternotes 

> These are things to think about that weren't required.

**Model solution**: Remember, this is a model solution. It is not the only solution. You may have included some of these things but not others. The purpose of this _model_ is to demonstrate the sort of discussions you should be having and how to articulate them in a reasonable report. Your decisions about what seems "relevant" are likely different than mine, and that's ok.

**Data splitting**: The procedure I used for model selection here deviates from the "workflow" discussed in class. The reason that I split the data in half and performed model selection on one half was to avoid using the same data to select the model and then produce confidence intervals and $p$-values. If I had done that, I would have _guaranteed_ that the CIs were wrong. In this way, if the selected model is _correct_, the CIs are correct. Whether or not the final model is correct is up for interpretation, but it's not obviously wrong (I don't think).

**EDA**: With very limited space, it is important to be selective about EDA,
and indeed everything else.  It is neither necessary nor desirable to include
in the report a histogram of every marginal distribution, a description of it,
a description of every part of the pairs plot, etc.  You should certainly
look at those, during your data analysis, but then you should select
the subset of plots which will give the reader a reasonable sense of
the data, and, more especially, those which actually made a difference
to your modeling decisions and analysis.

**Initial Model**: Here, I deliberately tried to keep the initial model simple,
with just what we'd need to answer the client's questions and a few other
things which seemed compelling from the EDA.  This then needs to be followed by
checking whether the variables or terms omitted mightn't have mattered.
Another tactic is to throwing everything in initially, and then try to prune
the model down.  Which one to attempt is largely a matter of taste.

**Hunting for Interactions**: The number of possible product interactions among
$p$ variables is $p(p-1)/2$.  This grows far too rapidly for manual examination
to be feasible, and even for conventional hypothesis testing.  (Throwing in
transformations makes things even worse.)  You should, therefore, be very
selective about hunting for transformations, trying as far as possible to base
them on either background knowledge, or things that look funny in the EDA, or
the diagnostics on the initial model.

**Multiplicative contributions**: A log-transformation of the price would lead
to a model where each term made a multiplicative, rather than an additive,
contribution to the price.  Another possibility, a bit trickier to manage with
our mathematical tool-kit, would be for price to be proportional to finished
area, but at a rate which is a function of the other predictor variables.  The
model would say, in effect, "high-quality houses built in 1985 near highways
sell for so many dollars per square foot" --- which sounds very much like a
realtor or an appraiser.

**Candidate Models**: If, when you are looking at your diagnostics and they
suggest a change which might improve the model but which is not obviously,
overwhelmingly the right thing to do, you have a choice, and each possible
choice expands your set of candidate models.  Some choices might not work well
together, and so you can narrow the pool of candidates by looking at further
diagnostic plots.  Here, however, I deliberately avoided that, instead just
taking all the combinations of the choices (32 of them!)  and letting
cross-validation sort it out.

**Avoiding model selection** An alternative to selecting _a_ model is to look
at many models, rejecting the ones where the assumptions are detectably
violated.  (You can think of the ones you retain as, almost, a confidence set
of models.)  You then report a range of predictions, estimates and inferences,
from across the retained models.  This requires some care (what, _exactly_, are
the model assumptions, and _just_ how badly can they fit the data before
rejection?), and can be harder to explain to clients than picking the
best-predicting model.

## Grading rubric

```{r grade-vector, echo=FALSE}
a = c(4, 2, 4, 5, 10, 0)
ab_print <- function(pos, modify){
  a <- a
  b <- a[pos]
  c <- b + modify
  a[pos] <- c
  a <<- a
  paste0(c,' / ', b)
}
```

__Words__ (`r ab_print(1,0)`) The text is laid out cleanly, with clear divisions
and transitions between sections and sub-sections. The writing itself
is well-organized, free of grammatical and other mechanical errors,
divided into complete sentences logically grouped into paragraphs and
sections, and easy to follow from the presumed level of knowledge. 

__Numbers__ (`r ab_print(2,0)`) All numerical results or summaries are reported to
suitable precision, and with appropriate measures of uncertainty
attached when applicable. 

__Pictures__ (`r ab_print(3,0)`) Figures and tables are easy to read, with
informative captions, axis labels and legends, and are placed near the
relevant pieces of text or referred to with convenient labels. 

__Code__ (`r ab_print(4,0)`) The code is formatted and organized so that it is easy
for others to read and understand. It is indented, commented, and uses
meaningful names. It only includes computations which are actually
needed to answer the analytical questions, and avoids redundancy. Code
borrowed from the notes, from books, or from resources found online is
explicitly acknowledged and sourced in the comments. Functions or
procedures not directly taken from the notes have accompanying tests
which check whether the code does what it is supposed to. The text of
the report is free of intrusive blocks of code. With regards to R Markdown,
all calculations are actually done in the file as it knits, and only
relevant results are shown.

__Analysis__ (`r ab_print(5,0)`) Variables are examined
individually and bivariately. Features/observations are discussed with
appropriate figure or tables. The relevance of the EDA to the modeling
is clearly explained. The  model's
formulation is clearly related to the substantive questions of
interest. The model's assumptions are checked by means of appropriate
diagnostic plots or formal tests; if the model is re-formulated, the
changes are both well-motivated by the diagnostics, and still allow
the model to answer the original substantive question. Limitations
from un-fixable problems are clearly noted. The substantive questions about real estate
pricing are answered as precisely as the data and the model
allow. The chain of reasoning from estimation results about models, or
derived quantities, to substantive con- clusions is both clear and
convincing. Contingent answers (``if $X$, then $Y$, but if $Z$, then $W$'') are
likewise described as warranted by the model and data. If
uncertainties in the data and model mean the answers to some questions
must be imprecise, this too is reflected in the discussion. 

__Extra credit__ (`r ab_print(6,0)`) Up to five points may be awarded for reports
which are unusually well-written, where the code is unusually elegant,
where the analytical methods are unusually insightful, or where the
analysis goes beyond the required set of analytical questions. 
